{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9913a4b5",
   "metadata": {},
   "source": [
    "Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0edf9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60b7c5",
   "metadata": {},
   "source": [
    "Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cb5c7f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = r\"C:\\Users\\afpue\\Documents\\GitHub\\NLP-Seminar\\publicaciones\\binaria_limpio.xlsx\"\n",
    "df = pd.read_excel(archivo)\n",
    "df['Clase'] = df['Clase'].map({'Negativo': 0, 'Positivo': 1})\n",
    "\n",
    "df['Documento'] = df['Documento_Lematizado']\n",
    "def limpiar(texto):\n",
    "    texto = texto.lower()\n",
    "    return re.findall(r'\\b\\w+\\b', texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42b8a1",
   "metadata": {},
   "source": [
    "Diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "28a239b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_positivas = {\"feliz\", \"alegre\", \"contento\", \"maravilloso\", \"excelente\", \"genial\", \"bueno\", \"positivo\", \"fantástico\", \"me encanta\",\n",
    "                      \"filantropos\", \"filantropo\"}\n",
    "palabras_negativas = {\"triste\", \"deprimido\", \"mal\", \"horrible\", \"terrible\", \"enfermo\", \"negativo\", \"odio\", \"me molesta\", \"estresado\", \n",
    "                      \"muertes\", \"muerte\", \"enfermedad\", \"dolor\", \"sufrimiento\", \"tragedia\", \"desastre\", \"crisis\", \"problema\", \"conflicto\",\n",
    "                      \"infectados\", \"desgracia\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd085f",
   "metadata": {},
   "source": [
    "Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0948fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_caracteristicas(tweet, palabras_negativas, palabras_positivas):\n",
    "    tokens = limpiar(tweet)\n",
    "    x0 = 1  # Bias\n",
    "    x1 = sum(1 for palabra in tokens if palabra in palabras_negativas)\n",
    "    x2 = sum(1 for palabra in tokens if palabra in palabras_positivas)\n",
    "    x3 = len(tokens)\n",
    "    return [x0, x1, x2, x3]\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "def dot(w, x):\n",
    "    return sum(wi * xi for wi, xi in zip(w, x))\n",
    "\n",
    "def predict_proba(w, x):\n",
    "    return sigmoid(dot(w, x))\n",
    "\n",
    "def single_log_loss(p, y):\n",
    "    p = max(min(p, 1 - 1e-15), 1e-15)\n",
    "    return - y * math.log(p) - (1 - y) * math.log(1 - p)\n",
    "\n",
    "def compute_batch_gradient(w, X_batch, y_batch):\n",
    "    grad = [0.0 for _ in w]\n",
    "    batch_size = len(y_batch)\n",
    "    for xi, yi in zip(X_batch, y_batch):\n",
    "        p = predict_proba(w, xi)\n",
    "        for j in range(len(w)):\n",
    "            grad[j] += (p - yi) * xi[j]\n",
    "    return [g / batch_size for g in grad]\n",
    "\n",
    "def train_logistic_regression_minibatch(X, y, lr=0.01, epochs=1000, batch_size=10):\n",
    "    w = [0.0 for _ in range(len(X[0]))]  # Inicializar pesos\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        combined = list(zip(X, y))\n",
    "        random.shuffle(combined)\n",
    "\n",
    "        # Dividir en mini-batches\n",
    "        for i in range(0, len(y), batch_size):\n",
    "            batch = combined[i:i + batch_size]\n",
    "            X_batch, y_batch = zip(*batch)\n",
    "\n",
    "            # Calcular gradiente promedio en el batch\n",
    "            grad = compute_batch_gradient(w, X_batch, y_batch)\n",
    "\n",
    "            # Actualizar pesos\n",
    "            w = [wj - lr * gj for wj, gj in zip(w, grad)]\n",
    "\n",
    "        # (Opcional) Calcular pérdida promedio en la época\n",
    "        loss_epoch = sum(single_log_loss(predict_proba(w, xi), yi) for xi, yi in zip(X, y)) / len(y)\n",
    "        if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Época {epoch}: pérdida promedio = {loss_epoch:.4f}\")\n",
    "\n",
    "    return w\n",
    "\n",
    "def predict(w, x, threshold=0.5):\n",
    "    return 1 if predict_proba(w, x) >= threshold else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7a6fd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_test = df.iloc[-1]['Documento']\n",
    "y_test = df.iloc[-1]['Clase']\n",
    "x_test = extraer_caracteristicas(tweet_test, palabras_negativas, palabras_positivas)\n",
    "\n",
    "# Resto como entrenamiento\n",
    "df_train = df.iloc[:-1]\n",
    "X_train = [extraer_caracteristicas(texto, palabras_negativas, palabras_positivas) for texto in df_train['Documento']]\n",
    "y_train = df_train['Clase'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "58ab1d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 0: pérdida promedio = 0.7275\n",
      "Época 100: pérdida promedio = 0.6592\n",
      "Época 200: pérdida promedio = 0.6535\n",
      "Época 300: pérdida promedio = 0.6386\n",
      "Época 400: pérdida promedio = 0.6116\n",
      "Época 500: pérdida promedio = 0.5830\n",
      "Época 600: pérdida promedio = 0.5814\n",
      "Época 700: pérdida promedio = 0.5579\n",
      "Época 800: pérdida promedio = 0.5489\n",
      "Época 900: pérdida promedio = 0.5581\n",
      "Época 999: pérdida promedio = 0.5322\n",
      "\n",
      "--- Resultado final ---\n",
      "Tweet: el mundo yo parecer más amable , más humano , menos raro .\n",
      "Clase real: 1\n",
      "Clase predicha: 1\n",
      "Probabilidad estimada de clase positiva: 0.6853\n"
     ]
    }
   ],
   "source": [
    "w = train_logistic_regression_minibatch(X_train, y_train, lr=0.01, epochs=1000, batch_size=5)\n",
    "\n",
    "# Predicción final sobre un ejemplo de prueba\n",
    "proba = predict_proba(w, x_test)\n",
    "clase_predicha = predict(w, x_test)\n",
    "\n",
    "print(\"\\n--- Resultado final ---\")\n",
    "print(\"Tweet:\", tweet_test)\n",
    "print(\"Clase real:\", y_test)\n",
    "print(\"Clase predicha:\", clase_predicha)\n",
    "print(f\"Probabilidad estimada de clase positiva: {proba:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "af2a2716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6468778796656445, -1.6846246375238016, 0.0, 0.013156821027996254]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
