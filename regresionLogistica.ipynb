{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9913a4b5",
   "metadata": {},
   "source": [
    "Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60b7c5",
   "metadata": {},
   "source": [
    "Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047f2cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = r\"C:\\Users\\afpue\\Documents\\Seminario NLP\\NLP\\publicaciones\\binaria.xlsx\"\n",
    "df = pd.read_excel(archivo)\n",
    "# Asegurar que las clases sean 0 (negativo) y 1 (positivo)\n",
    "df['Clase'] = df['Clase'].map({'negativo': 0, 'positivo': 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd085f",
   "metadata": {},
   "source": [
    "Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b118a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Función para limpiar datos ---\n",
    "def limpiar(texto):\n",
    "    texto = texto.lower()\n",
    "    return re.findall(r'\\b\\w+\\b', texto)\n",
    "\n",
    "# --- Función sigmoide ---\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "# --- Producto punto ---\n",
    "def dot(w, x):\n",
    "    return sum(w_i * x_i for w_i, x_i in zip(w, x))\n",
    "\n",
    "# --- Predicción de probabilidad ---\n",
    "def predict_proba(w, x):\n",
    "    return sigmoid(dot(w, x))\n",
    "\n",
    "# --- Pérdida logística (log-loss) promedio ---\n",
    "def log_loss(w, X, y):\n",
    "    loss = 0.0\n",
    "    for xi, yi in zip(X, y):\n",
    "        p = predict_proba(w, xi)\n",
    "        p = max(min(p, 1 - 1e-15), 1e-15)  # para evitar log(0)\n",
    "        loss += -yi * math.log(p) - (1 - yi) * math.log(1 - p)\n",
    "    return loss / len(y)\n",
    "\n",
    "# --- Gradiente del log-loss ---\n",
    "def compute_gradient(w, X, y):\n",
    "    grad = [0.0] * len(w)\n",
    "    n = len(y)\n",
    "    for xi, yi in zip(X, y):\n",
    "        p = predict_proba(w, xi)\n",
    "        for j in range(len(w)):\n",
    "            grad[j] += (p - yi) * xi[j]\n",
    "    return [g / n for g in grad]\n",
    "\n",
    "# --- Entrenamiento con descenso de gradiente ---\n",
    "def train_logistic_regression(X, y, lr=0.1, epochs=1000):\n",
    "    w = [random.uniform(-0.01, 0.01) for _ in range(len(X[0]))]\n",
    "    for epoch in range(epochs):\n",
    "        grad = compute_gradient(w, X, y)\n",
    "        w = [w_i - lr * g_i for w_i, g_i in zip(w, grad)]\n",
    "        if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "            loss = log_loss(w, X, y)\n",
    "            print(f\"Época {epoch}: pérdida = {loss:.4f}\")\n",
    "    return w\n",
    "\n",
    "# --- Predicción final binaria ---\n",
    "def predict(w, x, threshold=0.5):\n",
    "    return 1 if predict_proba(w, x) >= threshold else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486ec17",
   "metadata": {},
   "source": [
    "Caracteristicas / Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9daf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_positivas = {\"feliz\", \"alegre\", \"contento\", \"maravilloso\", \"excelente\", \"genial\", \"bueno\", \"positivo\", \"fantástico\", \"me encanta\"}\n",
    "palabras_negativas = {\"triste\", \"deprimido\", \"mal\", \"horrible\", \"terrible\", \"enfermo\", \"negativo\", \"odio\", \"me molesta\", \"estresado\", \n",
    "                      \"muertes\", \"muerte\", \"enfermedad\", \"dolor\", \"sufrimiento\", \"tragedia\", \"desastre\", \"crisis\", \"problema\", \"conflicto\",\n",
    "                      }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bdf091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_caracteristicas(tweet):\n",
    "    tokens = limpiar(tweet)\n",
    "    x0 = 1\n",
    "    x1 = sum(1 for palabra in tokens if palabra in palabras_positivas)\n",
    "    x2 = sum(1 for palabra in tokens if palabra in palabras_negativas)\n",
    "    x3 = len(tokens)\n",
    "    return [x0, x1, x2, x3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a444c6e",
   "metadata": {},
   "source": [
    "Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X será una lista de listas con las características\n",
    "X = [extraer_caracteristicas(t) for t in df['Documento']]\n",
    "# y será una lista con las clases: 0 = negativo, 1 = positivo\n",
    "y = df['Clase'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e1be5",
   "metadata": {},
   "source": [
    "Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88469666",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = train_logistic_regression(X, y, lr=0.01, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f829b",
   "metadata": {},
   "source": [
    "Prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_tweet = \"Me siento muy feliz y positivo hoy\"\n",
    "x_nuevo = extraer_caracteristicas(nuevo_tweet)\n",
    "prediccion = predict(w, x_nuevo)\n",
    "\n",
    "print(\"Predicción:\", prediccion)  # 1 si positivo, 0 si negativo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb14b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import random\n",
    "\n",
    "# === Definiciones ===\n",
    "palabras_felices = {\"feliz\", \"alegre\", \"contento\", \"maravilloso\", \"excelente\", \"genial\", \"bueno\", \"positivo\", \"fantástico\", \"encanta\"}\n",
    "palabras_tristes = {\"triste\", \"deprimido\", \"mal\", \"horrible\", \"terrible\", \"enfermo\", \"negativo\", \"odio\", \"molesta\", \"estresado\"}\n",
    "\n",
    "def limpiar(texto):\n",
    "    texto = str(texto).lower()\n",
    "    return re.findall(r'\\b\\w+\\b', texto)\n",
    "\n",
    "def extraer_caracteristicas(tweet):\n",
    "    tokens = limpiar(tweet)\n",
    "    x0 = 1\n",
    "    x1 = sum(1 for palabra in tokens if palabra in palabras_felices)\n",
    "    x2 = sum(1 for palabra in tokens if palabra in palabras_tristes)\n",
    "    x3 = len(tokens)\n",
    "    return [x0, x1, x2, x3]\n",
    "\n",
    "def sigmoid(z):\n",
    "    if z < -700:\n",
    "        return 0.0\n",
    "    elif z > 700:\n",
    "        return 1.0\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "def dot(w, x):\n",
    "    return sum(wi * xi for wi, xi in zip(w, x))\n",
    "\n",
    "def predict_proba(w, x):\n",
    "    return sigmoid(dot(w, x))\n",
    "\n",
    "def log_loss(w, X, y):\n",
    "    loss = 0.0\n",
    "    for xi, yi in zip(X, y):\n",
    "        p = predict_proba(w, xi)\n",
    "        p = max(min(p, 1 - 1e-15), 1e-15)\n",
    "        loss += -yi * math.log(p) - (1 - yi) * math.log(1 - p)\n",
    "    return loss / len(y)\n",
    "\n",
    "def compute_gradient(w, X, y):\n",
    "    grad = [0.0] * len(w)\n",
    "    n = len(y)\n",
    "    for xi, yi in zip(X, y):\n",
    "        p = predict_proba(w, xi)\n",
    "        for j in range(len(w)):\n",
    "            grad[j] += (p - yi) * xi[j]\n",
    "    return [g / n for g in grad]\n",
    "\n",
    "def train_logistic_regression(X, y, lr=0.01, epochs=1000):\n",
    "    w = [random.uniform(-0.01, 0.01) for _ in range(len(X[0]))]\n",
    "    for epoch in range(epochs):\n",
    "        grad = compute_gradient(w, X, y)\n",
    "        w = [wi - lr * gi for wi, gi in zip(w, grad)]\n",
    "        if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "            loss = log_loss(w, X, y)\n",
    "            print(f\"Época {epoch}: pérdida = {loss:.4f}\")\n",
    "    return w\n",
    "\n",
    "def predict(w, x, threshold=0.5):\n",
    "    return 1 if predict_proba(w, x) >= threshold else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6fd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leer archivo\n",
    "archivo = r\"C:\\Users\\afpue\\Documents\\Seminario NLP\\NLP\\publicaciones\\binaria.xlsx\"\n",
    "df = pd.read_excel(archivo)\n",
    "\n",
    "# Asegurar codificación binaria de clase\n",
    "df['Clase'] = df['Clase'].map({'Negativo': 0, 'Positivo': 1})\n",
    "\n",
    "# Último tweet como test\n",
    "tweet_test = df.iloc[-1]['Documento']\n",
    "y_test = df.iloc[-1]['Clase']\n",
    "x_test = extraer_caracteristicas(tweet_test)\n",
    "\n",
    "# Resto como entrenamiento\n",
    "df_train = df.iloc[:-1]\n",
    "X_train = [extraer_caracteristicas(texto) for texto in df_train['Documento']]\n",
    "y_train = df_train['Clase'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ab1d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 0: pérdida = 0.6871\n",
      "Época 100: pérdida = 0.6848\n",
      "Época 200: pérdida = 0.6826\n",
      "Época 300: pérdida = 0.6805\n",
      "Época 400: pérdida = 0.6786\n",
      "Época 500: pérdida = 0.6768\n",
      "Época 600: pérdida = 0.6751\n",
      "Época 700: pérdida = 0.6735\n",
      "Época 800: pérdida = 0.6720\n",
      "Época 900: pérdida = 0.6706\n",
      "Época 999: pérdida = 0.6693\n",
      "\n",
      "--- Resultado final ---\n",
      "Tweet: El mundo me parece más amable, más humano, menos raro. \n",
      "Clase real: 1\n",
      "Clase predicha: 1\n",
      "Probabilidad estimada de clase positiva: 0.5798\n"
     ]
    }
   ],
   "source": [
    "w = train_logistic_regression(X_train, y_train, lr=0.01, epochs=1000)\n",
    "\n",
    "# Predicción final\n",
    "proba = predict_proba(w, x_test)\n",
    "clase_predicha = predict(w, x_test)\n",
    "\n",
    "print(\"\\n--- Resultado final ---\")\n",
    "print(\"Tweet:\", tweet_test)\n",
    "print(\"Clase real:\", y_test)\n",
    "print(\"Clase predicha:\", clase_predicha)\n",
    "print(f\"Probabilidad estimada de clase positiva: {proba:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
