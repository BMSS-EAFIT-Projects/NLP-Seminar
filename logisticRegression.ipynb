{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9913a4b5",
   "metadata": {},
   "source": [
    "Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0edf9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60b7c5",
   "metadata": {},
   "source": [
    "Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb5c7f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = r\"C:\\Users\\afpue\\Documents\\GitHub\\NLP-Seminar\\publicaciones\\binaria_limpio.xlsx\"\n",
    "df = pd.read_excel(archivo)\n",
    "df['Clase'] = df['Clase'].map({'Negativo': 0, 'Positivo': 1})\n",
    "\n",
    "df['Documento'] = df['Documento_Lematizado']\n",
    "def limpiar(texto):\n",
    "    texto = texto.lower()\n",
    "    return re.findall(r'\\b\\w+\\b', texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42b8a1",
   "metadata": {},
   "source": [
    "Diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28a239b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_positivas = {\"feliz\", \"alegre\", \"contento\", \"maravilloso\", \"excelente\", \"genial\", \"bueno\", \"positivo\", \"fantástico\", \"me encanta\",\n",
    "                      \"filantropos\", \"filantropo\"}\n",
    "palabras_negativas = {\"triste\", \"deprimido\", \"mal\", \"horrible\", \"terrible\", \"enfermo\", \"negativo\", \"odio\", \"me molesta\", \"estresado\", \n",
    "                      \"muertes\", \"muerte\", \"enfermedad\", \"dolor\", \"sufrimiento\", \"tragedia\", \"desastre\", \"crisis\", \"problema\", \"conflicto\",\n",
    "                      \"infectados\", \"desgracia\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd085f",
   "metadata": {},
   "source": [
    "Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar(texto):\n",
    "\n",
    "    return texto.lower().split()\n",
    "\n",
    "# Se estraen las características de cada tweet, esto se puede personalizar como se desee\n",
    "def extraer_caracteristicas(tweet, palabras_negativas, palabras_positivas):\n",
    "    tokens = limpiar(tweet)\n",
    "    x0 = 1  # Bias\n",
    "    x1 = sum(1 for palabra in tokens if palabra in palabras_negativas)\n",
    "    x2 = sum(1 for palabra in tokens if palabra in palabras_positivas)\n",
    "    x3 = len(tokens)\n",
    "    return [x0, x1, x2, x3]\n",
    "\n",
    "# Funcion sigmoide\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "# Producto punto entre dos vectores\n",
    "def dot(w, x):\n",
    "    return sum(wi * xi for wi, xi in zip(w, x))\n",
    "\n",
    "# Predicción de probabilidad y^\n",
    "def predict_proba(w, x):\n",
    "    return sigmoid(dot(w, x))\n",
    "\n",
    "# Funcion de perdida\n",
    "def single_log_loss(p, y):\n",
    "    p = max(min(p, 1 - 1e-15), 1e-15)\n",
    "    return - y * math.log(p) - (1 - y) * math.log(1 - p)\n",
    "\n",
    "# Cálculo del gradiente\n",
    "def compute_gradient_example(w, xi, yi):\n",
    "    p = predict_proba(w, xi)\n",
    "    grad = [(p - yi) * xj for xj in xi]\n",
    "    return grad\n",
    "\n",
    "# Entrenamiento de regresión logística con descenso de gradiente estocástico\n",
    "def train_logistic_regression_sgd(X, y, lr=0.01, epochs=1000):\n",
    "    w = [0.0 for _ in range(len(X[0]))]  # Inicializar pesos en cero (como en el libro)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        combined = list(zip(X, y))\n",
    "        random.shuffle(combined)\n",
    "\n",
    "        for xi, yi in combined:\n",
    "            p = predict_proba(w, xi) # Predicción de probabilidad, y^\n",
    "            loss_i = single_log_loss(p, yi) # Pérdida de y^ respecto a y\n",
    "            grad = compute_gradient_example(w, xi, yi) # Cálculo del gradiente\n",
    "            w = [wj - lr * gj for wj, gj in zip(w, grad)] # Actualización de pesos\n",
    "\n",
    "        # Promedio de pérdida de la época, esto solo es informativo para saber como va el entrenamiento\n",
    "        if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Época {epoch}: pérdida último ejemplo = {loss_i:.4f}\")\n",
    "\n",
    "    return w\n",
    "\n",
    "# Predicción binaria, se puede ajustar el umbral segun la necesidad\n",
    "def predict(w, x, threshold=0.5):\n",
    "    return 1 if predict_proba(w, x) >= threshold else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6fd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleccion del dato de prueba\n",
    "tweet_test = df.iloc[-1]['Documento']\n",
    "y_test = df.iloc[-1]['Clase']\n",
    "x_test = extraer_caracteristicas(tweet_test, palabras_negativas, palabras_positivas)\n",
    "\n",
    "# Dejo el resto como entrenamiento\n",
    "df_train = df.iloc[:-1]\n",
    "X_train = [extraer_caracteristicas(texto, palabras_negativas, palabras_positivas) for texto in df_train['Documento']]\n",
    "y_train = df_train['Clase'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab1d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 0: pérdida último ejemplo = 0.3939\n",
      "Época 100: pérdida último ejemplo = 1.1934\n",
      "Época 200: pérdida último ejemplo = 0.1727\n",
      "Época 300: pérdida último ejemplo = 0.3284\n",
      "Época 400: pérdida último ejemplo = 0.2938\n",
      "Época 500: pérdida último ejemplo = 0.5512\n",
      "Época 600: pérdida último ejemplo = 2.8434\n",
      "Época 700: pérdida último ejemplo = 0.0081\n",
      "Época 800: pérdida último ejemplo = 0.4690\n",
      "Época 900: pérdida último ejemplo = 1.4548\n",
      "Época 999: pérdida último ejemplo = 2.9443\n",
      "\n",
      "--- Resultado final ---\n",
      "Tweet: el mundo yo parecer más amable , más humano , menos raro .\n",
      "Clase real: 1\n",
      "Clase predicha: 1\n",
      "Probabilidad estimada de clase positiva: 0.9828\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "w = train_logistic_regression_sgd(X_train, y_train, lr=0.01, epochs=1000)\n",
    "\n",
    "# Predicción final\n",
    "proba = predict_proba(w, x_test)\n",
    "clase_predicha = predict(w, x_test)\n",
    "\n",
    "# Resultado final\n",
    "print(\"\\n--- Resultado final ---\")\n",
    "print(\"Tweet:\", tweet_test)\n",
    "print(\"Clase real:\", y_test)\n",
    "print(\"Clase predicha:\", clase_predicha)\n",
    "print(f\"Probabilidad estimada de clase positiva: {proba:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af2a2716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2796314493293743, -5.444717875507482, 0.0, 0.21270798947939554]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
